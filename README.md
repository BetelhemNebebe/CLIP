# CLIP

This work is based on:
- [CLIP (Contrastive Language-Image Pre-Training)](https://github.com/openai/CLIP)
- [Wav2CLIP](https://github.com/descriptinc/lyrebird-wav2clip/tree/master)

### 1. Set up environment

**Creating the environment:**

`conda env create -f clip_env.yml`

**Activating the environment:**

`conda activate clip_env`

### 2. Set path to input data

Before running the code, set the path to the input video data folder [here](https://github.com/BetelhemNebebe/CLIP/blob/41e6155c3a67315a485203c41a9390813cf8d570/main.py#L20).

### 3. Run code

`python main.py`
